---
title: "Analyse de séries temporelles avec R"
author: "Alexis Gabadinho"
date: "2023-10-05"
header-includes:
- \titlegraphic{\centering \includegraphics[width=8cm]{igpde.jpeg}}

output:
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    number_sections: true
    keep_tex: true
    toc: true
    df_print: tibble
# geometry: margin=1in
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
summarytools::st_options(plain.ascii = FALSE, style = "rmarkdown")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "tiny","\n\n", x, "\n\n \\normalsize")
})
```

# Introduction

## Support de cours et exercices

- Ce support est écrit en R markdown (document texte incluant du code R exécuté dynamiquement)
- L'environnement RStudio sera utilisé lors de la formation
- Le fichier source du support (`Rmd`) sera fourni aux participant-e-s, et leur permettra d'exécuter le code contenu dans les diapositives sur leur ordinateur
- Plusieurs jeux de données contenant des séries macroéconomiques sont utilisées pour les exemples. Ces données sont disponibles sous la forme de fichiers `csv` ou fournies par certaines des librairies R utilisées
- De nombreuses formules mathématiques sont présentes dans les diapositives. Il n'est pas nécessaire de les comprendre, elles seront expliquées en détail lors de la formation

## Librairies R requises

- Les librairies suivantes doivent être installées:
  - `tidyverse` (il s'agit en fait d'une collection de librairies dont: `tibble`, `tidyr`, `ggplot2`)
  - `GGally` (ajout de fonctions à `ggplot2`)
  - `ggfortify` (ajout de fonctions à `ggplot2`)
  - `tsibble` (objets de type `tidy` pour le stockage de données temporelles)
  - `feasts` (description, décomposition, représentations graphiques de séries temporelles)
  - `fable`
  - `structchange` (tests de changement structurel)
  - `urca` (test de racine unitaire)
  - `vars` (modèles VAR et VEC)

# Environnement de travail (rappels)

## Le `tidyverse` et `ggplot`

- Pour importer et manipuler les données, on va utiliser principalement le [tidyverse](https://www.tidyverse.org/) une collection de librairies pour la science des données (data science)
- Les librairies partagent des structures de données, une philisophie 

```{r tidyverse_load, echo=TRUE}
library(tidyverse)
```
- Pour les graphiques nous utiliserons principalement la librairie `ggplot2`, avec le theme `minimal`

```{r ggplot_load, echo=TRUE}
library(ggplot2)
theme_set(theme_minimal())
```

## Données Nelson-Plosser - Introduction

- Nous allons utiliser les données 'Nelson-Plosser', provenant de l'article orignal: C. R. Nelson and C. I. Plosser (1982), [Trends and Random Walks in Macroeconomic Time Series. Journal of Monetary Economics](https://www.sciencedirect.com/science/article/abs/pii/0304393282900125?via%3Dihub)
- Le jeu de données contient 14 séries temporelles macroécononiques
- La longueur des séries est variable, mais elles se terminent toutes en 1988
- Le jeu de données est décrit [ici](http://korora.econ.yale.edu/phillips/data/np&enp.dat)


## Données Nelson-Plosser - Séries

- Les 14 séries temporelles: 
  - cpi = consumer price index
  - ip = industrial production
  - gnp.nom = nominal GNP (millions de dollars 1988)
  - vel = velocity
  - emp = employment
  - int.rate = interest rate
  - nom.wages = nominal wages
  - gnp.def = GNP deflator
  - money.stock = money stock
  - gnp.real = real GNP (millions de dollars 1958)
  - stock.prices = stock prices (S&P500)
  - gnp.capita = GNP per capita (dollars 1958)
  - real.wages = real wages
  - unemp = unemployment


## Données Nelson-Plosser - Importation

- La fonction `read.csv2` permet d'importer les données à partir du fichier csv
- A noter: dans le fichier csv, le séparateur de champ est une virgule et le séparateur décimal est un point (format européen)


```{r nelplo_read, echo=TRUE}
Nelson_Plosser <- read.csv2("../data/Nelson_Plosser.csv", header=TRUE, sep=",", dec = ".")
head(Nelson_Plosser)
```

## Données Nelson-Plosser - Statistiques descriptives

- Pour les statistiques descriptives de chaque variable on peut utiliser la fonction `descr()` de la librairie `summarytools`
```{r, echo=TRUE, message=FALSE}
library(summarytools)
statdesc <- descr(Nelson_Plosser %>% select(1:6))
```

## Données Nelson-Plosser - Statistiques descriptives

```{r pib_summary, echo=TRUE, results="asis"}
statdesc
```


## Données Nelson-Plosser - Matrice de corrélation

- La matrice de corrélation contient les coefficients de corrélation linéaire entre les variables prises 2 à 2

```{r nelplo_corr, echo=TRUE, tidy=TRUE}
datanum <- Nelson_Plosser %>% filter(year>1909) %>% select(1:7) 
cormat <- round(cor(datanum),2)
cormat
```

## Données Nelson-Plosser - Matrice de corrélation

```{r nelplo_corr_heatmap, echo=TRUE, message=FALSE, out.width="70%", fig.align='center'}
library(reshape2)
cormat %>% melt() %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_fill_distiller(palette = "Spectral", direction = 1)
```

## Données Nelson-Plosser - Pairplot

- On peut visualiser la distribution et la corrélation entre variables avec la fonction `ggpairs` de la librairie `GGally`
```{r GGally, echo=FALSE}
library(GGally)
```
- On sélectionne ici un sous-ensemble de variables pour limiter la taille du graphique

```{r, message=FALSE}
g <- Nelson_Plosser %>% 
  filter(year>=1909) %>%
  select(cpi, gnp.nom, gnp.capita, emp, unemp) %>% 
  ggpairs(upper = list(continuous = wrap("cor", size = 4)),
          lower = list(continuous = wrap("points", colour="blue", alpha=0.3, size=0.5)))
```


## Données Nelson-Plosser - Pairplot

```{r nelplo_pairplot_graph, echo=TRUE, out.width="70%", fig.align = 'center'}
g
```

## Données `retail` - Importation

- Série des ventes du commerce de détail et nourriture (Advance retail sales)
- Données mensuelles, en millions de dollars, non corrigées des variations saisonnières
- Source: FRED (Federal Reserve Bank Economic Data)
- On importe les données à partir du fichier csv
- A noter: dans le fichier csv, le séparateur décimal est un point

```{r retail_read, echo=TRUE}
retail <- read.csv2("../data/RSXFSN.csv", header=TRUE, sep=",", dec = ".")
head(retail)
```

## Données `retail` - Conversion de la date

- La date est au format `character`, il faut la convertir
```{r}
summary(retail)
```

- On utilise la fonction `as.Date`
```{r}
retail$DATE <- as.Date(retail$DATE,format="%Y-%m-%d")
summary(retail)
```

## Exercices 

- Le fichier `pib_fr.csv` contient la série temporelle du PIB et de ses composants depuis 1949 (valeurs aux prix courants - Source: INSEE) :
- Importez les données à partir du fichier csv
- Explorez les données avec des statistiques descriptives et des graphiques


# Librairies spécialisées et structures de séries temporelles dans R

## Liste des librairies

- En plus de la librairie standard `stats`, il existe plusieurs librairies R pour la manipulation et l'analyse des séries temporelles
  - [tsibble](https://www.rdocumentation.org/packages/tsibble/)
  - forecasts
  - feasts
  - fable
  - ggfortify
  - `tseries`
  - `zoo`
- Dans cette partie on se focalise sur les classes permettant de stocker les séries temporelles fournies par les librairies `stats` et `tsibble` 

## Les objets `ts` et `mts`

- La classe de base fournie par R pour représenter des séries temporelles s’appelle `ts` (ts = time series, série univariée) ou `mts` (mts = multiple time series, série multivariée)
- Cette classe est définie dans le package `stats`
- Elle concerne des séries temporelles qui sont échantillonnées à des périodes **équidistantes** dans le temps


##  Les objets `ts` et `mts` - Paramètres

- Les objets de classe `ts` ou `mts` possèdent trois paramètres caractéristiques :
  - `frequency`: nombre d’observations par unité de temps. Si l’unité de temps de la série est l’année, la valeur 4 correspond à des trimestres et la valeur 12 à des mois
  - `start`: date de début de la série temporelle (nombre unique ou vecteur de deux entiers représentant respectivement une unité temporelle (e.g. une année) et une subdivision de cette unité (mois ou trimestre selon la valeur du paramètre frequency)
  - `end`: date de fin de la série temporelle, exprimée comme pour le paramètre start


## Données `NelPlo` (Nelson-Plosser)

- Les données Nelson-Plosser sont également présentes dans la librairie `tseries`
- Les données annuelles (frequency=1) sont contenues dans un objet de type `mts`
```{r}
library(tseries)
data(NelPlo)
class(NelPlo)
```

## Données `NelPlo` - Aperçu

- La fonction `window` permet d’extraire une portion d’une série temporelle
- L'ensemble des séries est complet à partir de 1909 (il y a des valeurs manquantes sur certaines séries avant)
```{r}
window(NelPlo, start=1905, end=1910)
```

## Données `growthofmoney` (Growth of Money Supply)

- Deux séries temporelles concernant le contrôle de la monnaie par la réserve fédérale aux USA
- Article original: R.L. Hetzel (1981), [The Federal Reserve System and Control of the Money Supply in the 1970's](https://doi.org/10.2307/1991806). Journal of Money, Credit and Banking 13, 31--43
- Série temporelle multivariée, données trimestrielles

```{r gmoney_load, echo=TRUE, message=FALSE}
library(lmtest)
data(growthofmoney)
window(growthofmoney, end=c(1971,4))
```

## Données `USIncExp` (US Income and Expenditure)

- Deux séries macro-économiques (USA) de janvier 1959 à février 2001, corrigées des variations saisonnières:
  - Revenus personnels mensuels aggrégés (millions de dollars)
  - Dépences de consommation aggrégées (millions de dollars) 
```{r, message=FALSE, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
library(strucchange)
data("USIncExp")
window(USIncExp, start=c(1959,6), end=c(1960,6))
```


## Conversion des données `retail` en objet `ts`

- Pour les données retail, frequency=12 (mois)

```{r retail_ts, echo=TRUE}
retail_ts <- ts(retail['RSXFSN'], frequency=12, start=c(1992,1))
window(retail_ts, start=2020)
```

## Objets `ts` - Extraction de l'index

- La fonction `tsp` permet d'extraire les propriétés d'un objet `ts` ou `mts` 

```{r}
tsp(retail_ts)
```


- Extraction de l'index

```{r pib_ts_time, echo=TRUE}
retail2022 <- window(retail_ts, start=c(2022,1), end=c(2022,12))
time(retail2022)
```

- Conversion de l'index au format numérique
```{r pib_ts_time2, echo=TRUE}
library(lubridate)
as.numeric(time(retail2022))
```

## Les objets `tsibble`

- La librairie `tsibble` fournit une structure de type `tidy` pour les données temporelles ainsi que des outils pour le traitement de ces données 
- Les objets de la classe `tsibble` possèdent deux attributs principaux:
  - `index` est la variable qui représente le temps (qui permet d'ordonner du passé au présent)
  - `key` identifie (éventuellement) la série (unité d'observation)
- Chaque observation est identifiée de manière unique par la combinaison `index` et `key`


## Conversion des  données `growthofmoney`

- On utilise la fonction `as_tsibble()`

```{r gmoney_tsibble, echo=TRUE, message=FALSE}
library(tsibble)
gmoney_tsbl <- growthofmoney %>% as_tsibble()
gmoney_tsbl
```

## Conversion des données `NelPlo`

- La transformation d'un objet de type `mts` en objet `tsibble` produit un jeu de données au format long
```{r nelplo_tsibble, echo=TRUE, message=FALSE}
nelplo_tsbl <- NelPlo %>% as_tsibble()
nelplo_tsbl %>% filter(index>1980 & key=="gnp.capita")
```

## Conversion des données `NelPlo`

- Les 14 séries sont identifiées par la variable `key`
```{r}
nelplo_tsbl %>% distinct(key)
```

## Conversion des données `retail`

```{r retail_tsibble, echo=TRUE, message=FALSE}
retail_tsbl <- retail_ts %>% as_tsibble()
head(retail_tsbl)
```

## Exercice

- Convertissez les données PIB en objet `mts`
- Convertissez les données PIB en objet `tsibble`


# Définitions

## Série temporelle et processus stochastique

- Série temporelle univariée à temps discret = ensemble d'observations dans $\mathbb{R}$ enregistrées à un temps spécifique $t \in \mathbb{Z}$
- En statistique l'observation $x$ est considérée comme la réalisation d'une **variable aléatoire** $X$
- Une série temporelle $(x_t)_{t \in \mathbb{Z}}$  sera considérée comme la réalisation d'un **processus stochastique** $(X_t)_{t \in \mathbb{Z}}$
- Processus stochastique => pour tout $t \in \mathbb{Z}$  fixé, $X_t$  est une variable aléatoire réelle
- L'objectif est d'étudier les caractéristiques principales de ce processus (tendance, variation saisonnière), de le modéliser et de faire des prévisions

## Stationarité

- Dans de très nombreux cas, on ne peut pas renouveler la suite de mesures dans des conditions identiques
- Pour que le modèle déduit à partir d'une suite d'observations ait un sens, il faut que toute portion de la trajectoire observée fournisse des informations sur la loi du processus et que des portions différentes, mais de même longueur, fournissent les mêmes indications. D'où la notion de stationnarité.
- Un processus $(X_t)_{t \in \mathbb{Z}}$  est (faiblement) stationnaire si son espérance et ses autocovariances sont invariantes par translation dans le temps :
  - $\forall t \in \mathbb{Z}: \mathbb{E}(X_t) = \mu$
  - $\forall t \in \mathbb{Z}, \forall h \in \mathbb{Z}: Cov(X_t,X_{t-h})$ dépend de l'intervalle $h$, mais pas de $t$

## Fonction d'autocovariance et d'autocorrélation

- La fonction d'autocorrelation (ACF) est la corrélation entre $x_t$ et $x_{t-1}$, $x_{t-2}$, $x_{t-3}$, etc ... :
$$\rho_j = \frac{Cov(y_t,y_{t-j})}{\sqrt{Var(y_t)\cdot Var(y_{t-j})}}$$
- Elle permet d'identifier une structure dans la série temporelle


## Fonction d'autocorrélation avec `ACF()`

- Pour les objets `tsibble` on peut utiliser la fonction `ACF()` de la librairie `feasts`

```{r}
library(feasts)
nelplo_tsbl %>% 
  filter(key=="cpi") %>% 
  ACF(value)
```

- La corrélation entre le cpi à l'instant $t$ et le cpi à l'instant $t-1$ (une année avant) est de 0.966 


## Bruit blanc (white noise)

- Un bruit blanc ${\epsilon_t}$ est une série de variables aléatoires **non corrélées** de moyenne nulle et de variance constante
- $(\epsilon_t)_{t \in \mathbb{Z}}$ est un bruit blanc faible si :
  - Son espérance est égale à 0: $\forall t \in \mathbb{Z}: \mathbb{E}(\epsilon_{t})=0$
  - Sa variance est constante: $\mathbb{E}(\sigma^2_t) = \sigma^2$
  - La covariance entre $(\epsilon_t)$ et $(\epsilon_{t-h})$ est nulle : $\forall t \in \mathbb{Z}, \forall h \in \mathbb{Z}: Cov(\epsilon_t,\epsilon_{t-h})=0$
- Un bruit blanc gaussien ${\epsilon_t}$ est une série de variables aléatoires indépendantes et identiquement distribuées (i.i.d), suivant une loi normale de moyenne nulle et de variance $\sigma^2_z$ $N(0, \sigma^2_z)$
- Un bruit blanc est une série strictement stationnaire


## Bruit blanc (white noise) - Graphique

```{r, out.width="70%", fig.align="center"}
library(ggfortify)
autoplot(ts(rnorm(100)))+ 
  ggtitle("White Noise")
```

## Test de Portmanteau

- A l'issue d'une modélisation, il nous faudrait idéalement obtenir un signal résiduel qui ne contient plus d'information temporelle
- Dans le cadre des modèles ARMA, on souhaite que le résidu soit un bruit blanc (faible), c'est-à-dire sans dépendance temporelle linéaire
- On peut tester la **blancheur** d'une série $y_t, t= 1, \ldots, T$ en utilisant le test de Portmanteau
- La statistique de Portmanteau est calculée à partir les $k$ premiers coefficients d'autocorrélation
$Q_k = T \sum_{h=1}^k \hat{\rho}^2_j$ 
où $h$ est un décalage choisi par l’utilisateur et $\rho_j$ l’estimateur du coefficient d’autocorrélation d’ordre $j$ de la série $y_t$

## Test de Portmanteau - Exemple

- On peut utiliser les fonctions `box_pierce()` et `ljung_box()` (pour les petits échantillons) de la librairie `feasts`
- Hypothèse H0: pas d'autocorrélation

```{r}
box_pierce(rnorm(100))
```

- La p-valeur est supérieure à 0.05, on accepte H0
- A noter: quand le test est appliqué non sur des v.a. indépendantes, mais sur les résidus d’un ajustement estimant $m$ paramètres, on utilise le paramètre `dof` (degrees of freedom, degrés de liberté)

## Marche aléatoire (random walk)

- La série $y_t = y_{t-1} + \epsilon_t$ est une **marche aléatoire**
- Une série suivant une marche aléatoire prend, à deux dates consécutives, des valeurs proches et la variation par rapport à la date précédente est **indépendante du passé** (c'est un bruit blanc)
- En exprimant $y_{t-1}$ en fonction de $y_{t-2} , \ldots$ et d’une valeur initiale $y_0$ on obtient : $y_t = y_{0} + \epsilon_1 + \epsilon_2 + \ldots + \epsilon_t$
- Espérance: $E(y_t) = y_0$
- Variance: $var(y_t) = t \cdot \sigma_\epsilon^2$
- Covariance: $cov(y_t , y_{t+k}) = t \cdot \sigma_z^2 , (k > 0)$
- Il s'agit d'une série non-stationnaire car ni la variance ni l'autoccorélation ne sont constantes

## Marche aléatoire (random walk) - Simulation

- On simule une marche aléatoire avec un bruit blanc gaussien de moyenne 0 et d'écart type 1
```{r}
tmax <- 100
# ourDrift <- 0.005
wnoise <- rnorm(99, mean=0, sd=1)
y <- rep(0,100)

for (t in 2:tmax) {
  y[t] = y[t-1] + wnoise[t-1] 
}
rw <- tibble(time=1:tmax, y=y)
rw
```

## Marche aléatoire (random walk) - Graphique

```{r, out.width="70%", fig.align="center"}
ggplot(rw) + geom_line(aes(x=time, y=y), colour="blue")
```


## Marche aléatoire avec dérive (random walk with drift)

- La série $y_t = \alpha + y_{t-1} + \epsilon_t$ est une **marche aléatoire** avec dérive
- En exprimant $y_{t-1}$ en fonction de $y_{t-2} , \ldots$ et d’une valeur initiale $y_0$ on obtient : $y_t = \alpha \cdot t + y_{0} + \epsilon_1 + \epsilon_2 + \ldots + \epsilon_t$
- Espérance: $E(y_t) = \alpha \cdot t + y_0$
- Variance: $var(y_t) = t \cdot \sigma_z^2$
- Covariance: $cov(y_t , y_{t+k}) = t \cdot \sigma_z^2 , (k > 0)$


## Marche aléatoire avec dérive - Simulation

```{r}
tmax <- 100

wnoise <- rnorm(99, mean=0, sd=1)
y <- rep(0,100)
alpha <- 0.8

for (t in 2:tmax) {
  y[t] = alpha + y[t-1] + wnoise[t-1] 
}
rwd <- tibble(time=1:tmax, y=y)
rwd
```

## Marche aléatoire avec dérive: Graphique

 - Le graphique de $y_t$ en fonction du temps est donc celui d’une droite à laquelle est
superposée une marche aléatoire

```{r, out.width="70%", fig.align="center"}
ggplot(rwd) + geom_line(aes(x=time, y=y), colour="blue")
```

## Exercice

- Construisez une série $y_t$ représentant une marche aléatoire avec $y_0=1.39$ (données Nelson-Plosser) et $\sigma_\epsilon^2=0.41$
- Calculez les coefficients d'autocorrélation de la série
- Réalisez un test de Portmanteau sur la série, que concluez vous ?


# Analyse descriptive et représentations graphiques

## Représenter des séries temporelles: chronogramme

- Chronogramme: diagramme des points (x=date, y=valeur de l'observation)
- Pour un objet de la classe `ts` on peut utiliser la méthode générique `plot()`

```{r sunspot, echo=FALSE, out.width="70%", fig.align="center"}
plot(NelPlo[,c("cpi", "unemp")],xlab="Trimestre", main="Données Nelson-Plosser")
```

## Chronogramme - Données Nelson Plosser - Price index

```{r , out.width="70%", fig.align="center", warning=FALSE}
nelplo_tsbl %>% filter(key=="cpi") %>%
  ggplot(aes(x = index, y = value))+
    geom_line(color = "#00AFBB", size = 1)
```

## Chronogramme - Données `USIncExp`

```{r , out.width="70%", fig.align="center", warning=FALSE}
USIncExp %>% as_tsibble() %>%
  ggplot(aes(x = index, y = value))+
    geom_line(aes(color = key), size = 1)
```

## Chronogramme - Données `retail`

```{r , out.width="70%", fig.align="center", warning=FALSE}
retail_tsbl %>%
  ggplot(aes(x = index, y = value))+
    geom_line(color = "#00AFBB", size = 1)
```

## Chronogramme - Séries `growthofmoney`

```{r, warnings=F, message=FALSE, out.width="70%", fig.align="center"}
ggplot(gmoney_tsbl, aes(x = index, y = value)) + 
  geom_line(aes(color = key), size = 1) +
  scale_color_manual(values = c("red", "blue"))
```

## Séries du jeu de données `nelplo` (Nelson-Plosser)

```{r, warnings=F, message=FALSE, out.width="70%", fig.align="center"}
nelplo_tsbl %>% filter(key %in% c("gnp.nom", "gnp.real", "unemp", "cpi", "money.stock")) %>%
  ggplot(aes(x = index, y = value)) + 
    geom_line(aes(color = key), size = 1)
```


## Fonction d'autocorrélation - Données Nelson-Plosser - Price index

- On peut représenter simplement la fonction d'autocorrélation obtenue avec la fonction `ACF()`
```{r, warnings=F, message=FALSE, out.width="70%", fig.align='center'}
nelplo_tsbl %>% filter(key=="cpi") %>% 
  ACF(value) %>% autoplot()
```


## Représentation de la fonction d'autocorrélation - Données `growthofmoney`

```{r, warnings=F, message=FALSE, , out.width="70%", fig.align='center'}
gmoney_tsbl %>% ACF() %>% autoplot()
```

## Autocorrélation - Données `retail`

```{r, warnings=F, message=FALSE, , out.width="70%", fig.align='center'}
retail_tsbl %>% ACF() %>% autoplot()
```

## Season plot - Données `growthofmoney`

```{r, out.width='70%', fig.align='center', message=FALSE}
library(forecast)
ggseasonplot(window(growthofmoney[,"TG1.TG0"]))
```

## Season plot - Données `retail`

```{r, out.width='70%', fig.align='center'}
ggseasonplot(window(retail_ts, start=c(2000,1)))
```

## Lag plot - Données `retail`

- Le 'lag plot' représente la série temporelle et ses valeurs précédentes
```{r, out.width='70%', fig.align='center'}
retail_tsbl %>% gg_lag(y=value, geom="point", size=0.5, lags=c(1,3,6,9,12))
```

## Lag plot - Données Nelson-Plosser- Série `cpi`

```{r, out.width='70%', fig.align='center'}
nelplo_tsbl %>% filter(key=='cpi') %>% 
  gg_lag(y=value, geom="point", size=0.5, colour="blue", lags=c(1,2,5,10,15)) 
```

## Exercice

- Représentez les séries des données PIB (chronogramme, fonction d'autocorrélation, lag plot)


# Régression Linéaire

## Le modèle de régression linéaire simple

- Modèle de régression linéaire simple (une seule variable indépendante):
$$y_i = \beta_1 +\beta_2 \cdot x_i+\epsilon_i$$
  - Les observations sont indicées par $i$, $(i= 1, \ldots, N)$
  - $y_i$ est la variable **expliquée** (dépendante)
  - $x_i$ est la variable **explicative** (indépendante)
  - $\beta_1$ et $\beta_2$ sont les **paramètres** (à estimer)
  - $\epsilon_i$ est le **résidu** (écart aléatoire, erreur)
- L'équation de la droite de régression est déterminée par la pente (slope) ($\beta_0$) et l'intercept ($\beta_1$) :
$$E(y|x)=\beta_1+\beta_2 \cdot x$$

## Hypothèses de base du modèle linéaire

-  On parle de Moindres Carrés Ordinaires (MCO) ou Ordinary Least Square model (OLS) car l'objectif lors de l'estimation est de minimiser la somme des erreurs au carré $\sum_i \epsilon_i^2$
- Les hypothèses de base concernent la distribution de probabilité des résidus $\epsilon_i$:
  - Hypothèse 1 : $\epsilon_i$ suit une distribution normale $N(\mu, \sigma^2)$
  - Hypothèse 2 : l'espérance de $\epsilon_i$ est nulle : $\forall i, E(\epsilon_i)=0$
  - Hypothèse 3 : la variance de $\epsilon_i$ est constante (homoscédasticité): $\forall i, V(\epsilon_i)=\sigma^2$
  - Hypothèse 4 : la covariance entre deux observations est nulle: $\forall i \ne j,  Cov(\epsilon_i, \epsilon_j)=0$, il n'y a pas d'autocorrélation des résidus, ils sont sériellement indépendants


## Régression linéaire simple - Données Nelson-Plosser

- On considère les données Nelson-Plosser à partir de l'année 1909 (toutes les séries complètes)
- On utilise la fonction `spread` pour passer du format "long" au format "large"
```{r}
nelplo1909 <- nelplo_tsbl %>% 
  filter(index>=1909) %>% 
  spread(key = key, value=value)
nelplo1909
```

## Régression linéaire simple - Exemple 1

- On commence par un modèle bivarié:
  - variable expliquée $y$ = gnp.nom (PNB, millions de dollars)
  - variable explicative $x$ = emp (emploi, milliers de personnes)
- Note: on ne tient pas compte ici de l'aspect 'série temporelle' des données

```{r}
nelplo1909 %>% select(index, gnp.nom, emp)
```

## Régression linéaire simple - Visualisation

```{r, out.width="70%", fig.align='center'}
nelplo1909 %>%
  ggplot(aes(emp, gnp.nom)) +
    geom_point(colour="blue")
```

## Régression linéaire simple - Estimation

- L'estimation des paramètres se fait avec la fonction `lm` (Linear Models)

```{r}
mod1 <- lm(gnp.nom ~ emp, data=nelplo1909)
summary(mod1)
```

## Régression linéaire simple - Coefficients

- Un test avec H0: $\beta=0$ est réalisé pour chacun des coefficients
- Le coefficient associé à la variable explicative `emp` (emploi) est statistiquement significatif (p-valeur < 0.001)
- La valeur de $R^2$ (variance expliquée) est élevée

## Graphique de la droite de régression

```{r, out.width="70%", fig.align='center', message=FALSE}
ggplot(nelplo1909, aes(x=emp, y=gnp.nom)) +
  geom_point(colour="blue") +
  geom_smooth(method='lm', color="red")
```

## Valeurs observées, valeurs prédites, résidus

- Les valeurs prédites par le modèle se trouvent dans l'attribut `fitted.values`
- Les résidus représentent la différence valeur observée-valeur prédite, ils se trouvent dans l'attribut `residuals`
```{r}
mod1_diag <- tibble(gnp=nelplo1909$gnp.nom, predicted=mod1$fitted.values, residual=mod1$residuals)
mod1_diag
```

## Graphique valeurs prédites x observées 

```{r, out.width="70%", fig.align='center', echo=FALSE}
ggplot(mod1_diag) + 
  geom_point(aes(x=gnp, y=predicted), colour="blue") + 
  geom_abline(colour="grey")
```


## Distribution des résidus

```{r, out.width="70%", fig.align='center'}
ggplot(mod1_diag, aes(x=residual)) + 
  geom_histogram(aes(y = after_stat(density)), fill="lightblue", binwidth = 0.05) +
  geom_density(aes(x=residual))
```

## Normalité des résidus

- Pour tester la normalité des résidus on utilise le test de **Shapiro-Wilk**
- Hypothèse H0: les résidus suivent une distribution normale
- La p-valeur est inférieure à 0.05, on rejette H0: les résidus ne sont pas distribués normalement

```{r}
shapiro.test(mod1_diag$residual)
```

## Normalité des résidus - Quantile-Quantile plot

```{r, out.width="70%", fig.align='center'}
qqnorm(mod1_diag$residual)
```

## Résidus vs valeurs prédites

```{r, out.width="70%", fig.align='center'}
ggplot(mod1_diag) + 
  geom_point(aes(x=predicted, y=residual), colour="blue")
```

## Hétéroscedasticité

- L'homoscédasticité des résidus est une des hypothèses fondamentales du modèle OLS/MCO
- Homoscédasticité = la variance des résidus est constante:
$$Var(\epsilon_i) = \sigma^2$$
- Hétéroscédasticité = la variance des résidus n'est pas constante:
$$Var(\epsilon_i) = \sigma_i^2$$
- Si l'hypothèse d'homoscédasticité des résidus est violée, les tests d'hypothèse sur les coéfficients $\beta$ du modèle OLS ne sont plus valides


## Tester l'homoscédasticité

- Le test de **Breusch-Pagan-Godfrey** permet de tester l'homoscédasticité des résidus
- Hypothèse nulle (H0): homoscédasticité (les résidus ont une variance constante)
- On utilise la function `bptest()` de la librairie R `lmtest`
- La p-valeur du test est supérieure à 0.05, on accepte H0

```{r bptest}
library(lmtest)
bptest(mod1)
```

## Graphique de la série des résidus

```{r}
gdata <- tibble(index=1:nrow(mod1_diag), residus=mod1_diag$residual)
gdata %>% ggplot() + geom_point(aes(x=index, y=residus), color="red")
```


## Autocorrélation des résidus - ACF 

- On peut utiliser un graphique `acf` pour visualiser la fonction d'autocorrélation des résidus

```{r, out.width="70%", fig.align='center'}
acf(mod1_diag$residual)
```

## Le test de Durbin-Watson

- Le test de **Durbin-Watson** est un test d’absence d’autocorrélation d’ordre 1 sur les
résidus d’une régression linéaire
- On utilise la fonction `dwtest` de la librairie `lmtest`
- Hypothèse nulle (H0): pas d'autocorrélation des résidus

```{r}
dwtest(mod1)
```

- Ici on rejette l'hypothèse H0, il y a autocorrélation des résidus


## Régression linéaire simple - Exemple 2

- Modèle de régression sur les données `growthofmoney`
- TG1.TG0: difference of current and preceding target for the growth rate of the money supply
- AG0.TG0: difference of actual growth rate and target growth rate for the preceding period

```{r, lm_gom_data}
data("growthofmoney")
head(growthofmoney)
```

## Régression linéaire simple - Données `growthofmoney`


```{r lm_gom_plot, out.width='70%', fig.align='center', message=FALSE}
gdata <- tibble(AG0_TG0=growthofmoney[,"AG0.TG0"], TG1_TG0=growthofmoney[,"TG1.TG0"])
ggplot(gdata) + geom_point(aes(x=AG0_TG0, y=TG1_TG0), color="blue")
```


## Régression linéaire simple - Données `growthofmoney`

- Estimation du modèle (utilisé par Hetzel dans son article)
```{r}
modelHetzel <- TG1.TG0 ~ AG0.TG0
gom.mod1 <- lm(modelHetzel, data=growthofmoney)
summary(gom.mod1)
```

## Graphique de la série des résidus - Données `growthofmoney`

```{r, out.width='70%', fig.align='center', message=FALSE}
gdata <- tibble(index=1:nrow(growthofmoney), residus=gom.mod1$residual)
gdata %>% ggplot() + geom_point(aes(x=index, y=residus), color="red")
```

## Autocorrélation des résidus - Données `growthofmoney`

```{r, out.width="70%", fig.align='center'}
acf(gdata$residus)
```

## Autocorrélation des résidus - Données `growthofmoney`

- La p-valeur du test de Durbin-Watson est largement supérieure à 0.05, on accepte H0, il n'y a pas d'autocorrélation des résidus

```{r}
dwtest(modelHetzel, data=growthofmoney)
```


## Test de changement structurel

- Le test de **Chow** permet d'identifier un éventuel changement structurel dans les données
- Le rejet de l'hypotèse H0 signifie qu'un meilleur ajustement peut être obtenu avec deux droites de régression  (i.e. les paramètres du modèle ne sont pas stables)
- On utilise la librairie `strucchange` (voir [article]( https://www.jstatsoft.org/article/view/v007i02))
- Pour ce test on doit indiquer la date du changement structurel à priori (ou l'index du point)

## Test de Chow - Données `growthofmoney`



## Test de changement structurel - Résultats

- Pour la série du PNB nominal on teste un changement structurel en 1933

```{r}
nelplo1909 %>% 
  mutate(point=1:nrow(nelplo1909)) %>%
  filter(index>=1929 & index < 1935) %>% select(point, index, gnp.nom)
```

- La p-valeur du test est inférieure à 0.05, on rejette H0

```{r}
library(strucchange)
model <- gnp.nom ~ index
sctest(model, point=25, data=nelplo1909, type="Chow")
```

## Test de changement structurel - Extension

- La librairie `strucchange` propose une extension du test de Chow: le changement structurel est recherché sur un intervalle
- Il faut préparer les données: ici la régression inclut le temps comme variable explicative, l'année doit être présente comme série dans l'objet `ts`
```{r}
model <- gnp.nom ~ year

tdata <- Nelson_Plosser %>% 
  filter(year>=1909)
tdata <- ts(tdata, start=1909)
```

## Test de changement structurel - Extension

- La statistique F est calculée pour chacun des points de l'intervalle
- Si sa valeur est élevée (supérieure au seuil de 5% représenté par la linge rouge) 
```{r, out.width="70%", fig.aling='center'}
fs <- Fstats(model, from = 10, to = 60, data = tdata)
plot(fs)
```

## Modèle de régression multivarié

- Modèle de régression multivarié = plusieurs variables indépendantes:
$$y=\beta_0+\beta_1 \cdot x_1+\beta_2 \cdot x_2 + \dots + \beta_k \cdot x_k + \epsilon$$

## Modèle de régression multivarié - Données Nelson-Plosser

- On souhaite prédire le PNB (`gnp.nom`, millions de dollars) par le taux de chômage, l'indice des prix et l'année
- On utilise la fonction `spread` pour passer du format "long" au format "large"
```{r}
nelplo1909 <- nelplo_tsbl %>% 
  filter(index>=1909) %>% 
  spread(key = key, value=value)
regdata <- nelplo1909 %>% 
  select(index, gnp.capita, unemp, cpi)
```


## Modèle de régression multivarié - Estimation

```{r}
mod2 <- lm(gnp.capita ~ index+unemp+cpi, data=regdata)
summary(mod2)
```

## Sélection du modèle

- On élimine les variables dont le coefficient n'est pas significativement différent de 0 (p-valeur > 0.05)
```{r}
mod3 <- lm(gnp.capita ~ index+unemp, data=regdata)
summary(mod3)
```

## Comparaison de modèles

- Pour comparer des modèles imbriqués, on peut utiliser un critère d'information (AIC ou BIC)

```{r}
AIC(mod1, mod2, mod3)
```

- On retient le modèle ayant le plus faible AIC, ici le modèle 3

## Exercice

- Analysez les résidus de la régression

```{r}
mod3 <- lm(gnp.capita ~ index+unemp, data=regdata)
```

- Les résidus sont-ils normalement distribués ?
- Sont-ils autocorrélés ?
- Leur variance est-elle constante ?


# Décomposition d'une série temporelle (moyennes mobiles)

## Composants d'une série temporelle

- Une série temporelle peut être décomposée en trois éléments:
  - Tendence
  - Saisonnalité
  - Résidus

## Décomposition avec la librairie `feasts`

- La librarie `feasts` propose deux méthodes de décomposition (classique et STL)
- La décomposition classique utilise les moyennes mobiles, la saisonalité peut être additive ou multiplicative
```{r}
dcmp <- retail_tsbl %>%
  model(classical_decomposition(value))
components(dcmp)
```

## Décomposition avec la librairie `feasts` - Graphique

```{r,  out.width='70%', fig.align='center'}
components(dcmp) %>% autoplot()
```

## Décomposition avec la librairie `feasts` - Graphique

```{r,  out.width='70%', fig.align='center'}
components(dcmp) %>% ggplot() + 
  geom_line(aes(x=index,y=value), colour="blue") +
  geom_line(aes(x=index,y=season_adjust), colour="red")
```



# Transformation des données et stabilisation de la variance

## Tendance linéaire

- Une série temporelle dont l'évolution est une fonction **déterministe** du temps est non-sationnaire
- Une série dont l’évolution autour d’une fonction déterministe du temps est stationnaire est dite stationnaire à une tendance près (trend stationary)
- On peut décrire une tendance linéaire par le modèle 
$$y_t = \beta_0 + \beta_1 \cdot t + \epsilon_t$$
- $y_t$ est non-stationnaire si $\beta1 \ne 0$, la moyenne de $y_t$ est $$\mu_t = \beta_0 + \beta_1 \cdot t$$ 

## Tendance linéaire - Test

- On peut tester l'hypothèse $\beta1 \ne 0$ (existence d'une tendance) en ajustant un modèle MCO sur les données (cette approche est valide si les erreurs $\epsilon_t$ sont un bruit blanc non corrélé)
- On peut ajuster un modèle pour estimer $\beta_0$ et $\beta_1$ puis analyser les résidus comme un processus stationnaire $\epsilon_t = y_t - \beta_0 - \beta_1 \cdot t$


## Tendance linéaire - Exemple

- Données `USIncExp`, dépenses aggrégées de consommation en millions de dollars

```{r, out.width="60%", fig.align='center', message=FALSE}
USIncExp %>% as_tsibble() %>% filter(key=='expenditure') %>%
  ggplot(aes(x = index, y = value)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method='lm', color="red")
```


## Tendance linéaire - Estimation du modèle

- Estimation du modèle par MCO
```{r}
regdata <- USIncExp %>% as_tsibble() %>% filter(key=='expenditure')
summary(lm(value ~ index, data=regdata))
```
- Le coefficient associé au temps (index) est significativement différent de 0 (p-valeur < 0.05)

## Tendance exponentielle

- Si la tendance est exponentielle, on peut ramener à une tendance linéaire en utilisant le log:
$$\log(y_t) = \beta_0 + \beta_1 \cdot t + \epsilon_t$$
 - Note: $\beta_1$ dans le modèle à tendance exponentielle est le taux de croissance annuel moyen (si $t$ est exprimé en années)


## Tendance exponentielle - Echelle logarithmique

```{r, out.width="60%", fig.align='center', message=FALSE}
USIncExp %>% as_tsibble() %>% filter(key=='expenditure') %>%
  ggplot(aes(x = index, y = log(value))) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method='lm', color="red")
```

## Tendance exponentielle - Modèle linéaire

```{r}
regdata <- USIncExp %>% as_tsibble() %>% 
  filter(key=='expenditure') %>%
  mutate(value=log(value))
summary(lm(value ~ index, data=regdata))
```

## Utilisation de la transformation logarithmique

- On utilise souvent le logarithme (naturel) d'une série temporelle pour l'analyse
- Par exemple dans l'article original de Nelson et Plosser, toutes les séries à l'exception de la série `stock.price` (prix des actions) sont transformées en log: 
"*The tendency of economic time series to exhibit variation that increases in mean and dispersion in proportion to absolute level motivates the transformation to natural logs and the assumption that trends are linear in the transformed data.*"
- L'utilisation du log d'une série:
  - diminue donc l'hétéroscédasticité (stabilisation de la variance)
  - transforme une tendance exponentielle en tendance linéaire


## Désaisonnalisation

- De nombreuses séries économiques présentent des comportements périodiques, rendant difficile la comparaison de deux instants successifs
- Cela peut être le cas particulièrement lorsque la série est trimestrielle ou mensuelle (données `retail`) 
- Le recours à une **désaisonnalisation** permet d'obtenir des séries dites corrigées des variations saisonnières (CVS)


## Désaisonnalisation par la régression linéaire

- On inclus la saisonnalité dans un modèle linéaire tendenciel avec des variables `dummy` (0 ou 1) représentant les mois, trimestres, etc US.
- Par exemple pour des trimestres on ajoute $4-1=3$ variables `dummy` (modèle avec constante):
$$y_t=\beta_0+\delta_1 \cdot Q1_t+\delta_2 \cdot Q2_t+\delta_3 \cdot Q3_t+\beta_1 \cdot t+\epsilon_t$$

## Désaisonnalisation - Exemple

- Données `retail` sur le commerce de détail
- On créé une variable catégorielle (factor) pour le mois
- Le temps est un index t de 1 à 381 (nombre de mois dans la série)
- Dans la formule on ajoute -1 pour un modèle sans constante (intercept)

```{r}
retail$mois <- factor(month(retail$DATE), labels=month(1:12, label=TRUE))
retail$t <- c(1:nrow(retail))
head(retail)
```

## Désaisonnalisation - Résultat

```{r}
modst <- lm(RSXFSN ~ t+ mois-1, data=retail)
summary(modst)
```


## Désaisonnalisation avec la régression linéaire: Prédiction


```{r,  out.width='70%', fig.align='center'}
retail$prediction <- predict.lm(modst)
ggplot(retail)+
  geom_line(mapping=aes(x=t,y=RSXFSN),color="blue")+
  geom_line(mapping=aes(x=t,y=prediction), color="red")
```

## Désaisonnalisation avec la régression linéaire: Calcul

- Creation des variables indicatrices pour le mois (`dummies`)
```{r}
retail_1992_2022 <- retail %>% filter(year(DATE)<2023)
annees = nrow(retail_1992_2022)/12
t=1:annees

for (i in 1:12)
{
  su=rep(0,times=12)
  su[i]=1
  s=rep(su,times=annees)
  assign(paste("s",i,sep=""),s)
}
cbind(retail_1992_2022[,"RSXFSN"],s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12)[1:12,]
```

## Désaisonnalisation avec la régression linéaire: Calcul

- Pour obtenir les données CVS on extrait les coefficients
```{r}
coefst <- modst$coefficients
coefst
```

```{r}
a <- mean(coefst[2:13])
b <- coefst[1]
c <- coefst[2:13]-mean(coefst[2:13])
y_cvs <- retail_1992_2022$RSXFSN-(c[1]*s1+c[2]*s2+c[3]*s3+c[4]*s4+c[5]*s5+c[6]*s6+c[7]*s7+c[8]*s8+c[9]*s9+c[10]*s10+c[11]*s11+c[12]*s12)
```


## Désaisonnalisation avec la régression linéaire: Données CVS

```{r,  out.width='60%', fig.align='center'}
gdata <- tibble(t=retail_1992_2022$t, original=retail_1992_2022$RSXFSN, cvs = y_cvs)

ggplot(gdata)+
  geom_line(mapping=aes(x=t,y=original),color="blue")+
  geom_line(mapping=aes(x=t,y=cvs), color="red")
```


## Désaisonnalisation par la régression linéaire (librairie `forecast`)

- On peut également utiliser la fonction `tslm` de la librairie `forecast` 
- Cette fonction ajuste un modèle linéaire incluant la saisonalité et la tendance (et éventuellement la tendance au carré)
- Modèle linéaire avec saisonnalité et tendance - données `retail`

```{r}
library(forecast)
bhat = tslm(retail_ts~trend+I(trend^2)+season)
summary(bhat)
```

## Stationarisation

- Une série est dite intégrée d’ordre $d$, notée $I(d)$, s’il faut la diﬀérencier $d$ fois pour obtenir une série stationnaire
- La fonction `diff()` permet d'obtenir la série intégrée
- La série intégrée débute au temps $t+1$

```{r, fig.height=3, fig.width=8}
cpi_diff <- diff(NelPlo[,"cpi"],1)

cpi_diff %>% as_tsibble() %>%
  ggplot(aes(x = index, y = value)) + 
    geom_line(colour="red", size = 1)
```

## Test de non-stationarité - ADF

- Le test ADF (Augmented Dickey-Fuller) permet de tester la non-stationarité d'une série 
- Avant de réaliser un test de non-stationnarité, il faut examiner le chronogramme de la série pour voir si la série présente une tendance, soit stochastique (marche aléatoire avec dérive), soit déterministe (série stationnaire à un trend déterministe près)

## Série présentant une tendance

- La série du log des dépences de consommation aux USA présente une tendance

```{r, out.width="70%", fig.align="center"}
USExp <- USIncExp %>% as_tsibble() %>% filter(key=='expenditure') %>%
  mutate(logExp=log(value))
ggplot(USExp, aes(x = index, y = logExp)) +
  geom_line(color = "blue", size = 1)
```

## Test ADF avec la fonction `urf.df` (librairie `urca`)

- Nous allons tester les hypothèses: 
  - Hypothèse nulle H0: la série est non stationnaire avec dérive
  - Hypothèse alternative H1: la série est stationnaire avec trend déterministe
- Le test considère un modèle autorégressif d'ordre p (p repésenté par le lag est inconnu)
- On choisi pour commencer une valeur de $p$ élevée et on retient la première valeur du lag fortement significative
```{r}
library(urca)
exp.df <- ur.df(y=as.data.frame(USExp)[,"logExp"], lags=12, type='trend')
exp.df
```

## Test ADF: Résultats

- La première valeur $p$ fortement significative est 8
```{r}
summary(exp.df)
```

## Test ADF: Résultats

- Résultat du test avec $p=8$
- La statistique qui nous intéresse est `tau3`, la valeur est supérieure au seuil de 10%, on retient l'hypothèse H0, la série ne présente pas de tendance déterministe et peut être rendue stationaire par différentiation

```{r}
exp.df <- ur.df(y=as.data.frame(USExp)[,"logExp"], lags=8, type='trend')
summary(exp.df)
```

## Série différenciée des dépenses de consommation aux USA

```{r, out.width="70%", fig.align="center"}
USExp_diff <- tibble(time=1:(nrow(USExp)-1), diff= diff(as.data.frame(USExp)[, "logExp"]))
ggplot(USExp_diff, aes(x = time, y = diff)) +
  geom_line(color = "blue", size = 1)
```


## Série ne présentant pas de tendance

- La série du taux de chômage aux USA ne présente pas de tendance

```{r, out.width="70%", fig.align="center"}
Unemp <- nelplo_tsbl %>% filter(key=='unemp' & index >= 1909) 
ggplot(Unemp, aes(x = index, y = value)) +
  geom_line(color = "blue", size = 1)
```

## Série ne présentant pas de tendance - Test ADF

- Après un premier test avec `lags=6`, le premier lag singificatif est 1
- On regarde la statistique `tau2`, ici elle est inférieure à la valeur critique à 1% donc on rejete H0 et on conclut à la stationarité de la série

```{r}
unemp.df <- ur.df(y=as.data.frame(Unemp)[,"value"], lags=1, type='drift')
summary(unemp.df)
```

## Test de stationarité - KPSS

- Le test KPSS proposé par Kwiatkowski est disponible dans les librairies `urca` et `tseries`
- Hypothèse nulle H0: la série est stationnaire (soit à une tendance près, soit à une moyenne non nulle près)
- Le test suppose que la série est la somme d’une marche aléatoire $R_t$, d’un trend déterministe et d’une erreur stationnaire $U_t$: $$y_t = R_t + \beta_1 + \beta_2 t + U_t$$ où $R_t = R_{t-1} + z_t$
- Pour tester que la série $y_t$ est stationnaire à une tendance près, l’hypothèse nulle est $\sigma_z^2 = 0$

## Test de stationarité - KPSS - Série `USExp`

- On teste ici l'hypothèse H0: la série des dépenses de consommation aux USA est stationaire à une tendance près (option `type="tau"')

```{r}
ktest <- ur.kpss(USExp$logExp, type="tau", lags="long")
summary(ktest)
```

- La valeur de la statistique est nettement supérieure à la valeur critique au seuil de 1%, on rejette H0, la série ne présente pas de tendance linéaire

## Test de stationarité - KPSS - Série `USExp`

- On teste maintenant l'hypothèse H0: la série des dépenses de consommation aux USA est stationaire de moyenne constante (option `type="mu"')

```{r}
ktest <- ur.kpss(USExp$logExp, type="mu", lags="long")
summary(ktest)
```
- La valeur de la statistique est nettement supérieure à la valeur critique au seuil de 1%, on rejette H0, la série n'est pas stationnaire

## Test de stationarité - KPSS - Série `USExp` intégrée

- On teste maintenant la stationarité de la série intégrée

```{r}
ktest <- ur.kpss(USExp_diff$diff, type="mu", lags="long")
summary(ktest)
```
- La valeur de la statistique se situe entre les valeurs critiques à 2.5% et 5%

## Exercices

- La série du PIB français présente t-elle une tendance ?
- Cette tendance est-elle déterministe ou stochastique ?
- Appliquez le test de stationarité approprié 


# Modélisation de séries temporelles stationnaires

## Introduction

- Les modèles AR (AutoRegressive), MA (Moving Average) et ARMA (AutoRegressive Moving Average) sont des modèles fondamentaux pour étudier et décrire le comportement des séries temporelles
- Ces modèles ont été popularisés par George Box et Gwilym Jenkins
- Leur principale limitation est qu'ils ne peuvent modéliser que des séries stationnaires, cependant, on peut transformer des séries non-stationnaires par différenciation pour les étudier avec ce cadre (modèles ARIMA)


## Modèle autorégressif (AR)

- Dans un modèle auto régressif, les variables explicatives sont des valeurs passées (lags) de la variable expliquée
- Dans un modèle AR(1) (autorégressif d'ordre 1), $y$ est retardé d'une période

$$y_t=\alpha+\beta_1 \cdot y_{t-1}+\epsilon_t$$
avec $\epsilon_t \sim N(0,\sigma^2)$
- Pour un modèle AR(1) stationnaire, $|\beta_1|<1$ 
- Propriétés d'un processus AR(1):
  - Moyenne de $y_t$: $\mu = \frac{\alpha}{1-\beta_1}$
  - Variance: $Var(x_t) = \frac{\sigma^2_w}{1-\beta_1^2}$
  - Corrélation: $\rho_h = \beta^h_1$

## Modèle autorégressif - Données Nelson-Plosser

- On peut visualiser $y$ x $y_{t-1}$ à l'aide de la fonction `gg_lag()`
```{r, echo=FALSE, , out.width="70%", fig.align='center'}
nelplo1909p <- nelplo_tsbl %>% filter(index>1909)
nelplo1909p %>% filter(key=="gnp.capita") %>%
  gg_lag(y=value, lags=1, geom="point", colour="blue")
```

## Modèle autorégressif - Données Nelson-Plosser - ACF

- Fonction d'autocorrélation avec la fonction `ACF`:
```{r,echo=FALSE}
nelplo1909p %>% filter(key=="gnp.capita") %>% ACF(value)
```
- La corrélation entre $y$ et $y_{t-1}$ est forte


## Modèle autorégressif - Estimation - Données Nelson-Plosser

- On peut estimer le modèle AR(1) avec la fonction `lm()` (modèle de régression linéaire):
```{r}
rdata <- nelplo1909p %>% filter(key=="gnp.capita") 
summary(lm(value ~ lag(value), data=rdata))
```
- Le coefficient associé à $y_{t-1}$ est significatif, la valeur de $R^2$ est élevée 


## Modèle moyenne mobile

- $y_t$ est un processus moyenne mobile d'ordre $q$ noté MA(q) si
$$y_t = \mu + \epsilon_t + \theta_1 \cdot \epsilon_{t-1} + \ldots + \theta_q \cdot \epsilon_{t-q}$$

- On considère que le processus est la résultante d'une combinaison linéaire de perturbations decorrélées (un bruit blanc et son passé)
- Un MA(q) est toujours stationnaire quelles que soient les valeurs des $\theta$
- Les propriétés d'un modèle MA(1) $$y_t = \mu + \epsilon_t + \theta_1 \cdot \epsilon_{t-1}$$
- $E[y_t] = \mu$
- $Var(y_t) = \sigma^2_\epsilon(1+\theta_1^2)$
- ACF is $\rho_1 = \theta_1/(1+\theta_1^2)$ and $\rho_h = 0$ for $h \geq 2$


## Modèle ARMA

- Un processus ARMA (Auto Regressive Moving Average) est une synthèse des processus AR et MA
- $y_t$ obéit à un modèle ARMA(p, q) s’il est stationnaire et vériﬁe :

$$y_t = c + \phi_1 \cdot y_{t-1} + \ldots + \phi_p \cdot y_{t-p} + \epsilon_t + \theta_1 \cdot \epsilon_{t-1} + \ldots + \theta_q \cdot \epsilon_{t-q}$$

## Identification d'un modèle ARMA

- Soit la trajectoire observée $y_1, \ldots , y_t$ d’une série $y_t$, éventuellement transformée par passage en log
- Si cette trajectoire peut-être considérée comme stationnaire, on peut lui ajuster un modèle ARMA(p, q) (on ne traite pas ici des séries présentant une saisonnalité)
- La première étape consiste à choisir les ordres $p$ et $q$ 
- Le choix de $p$ et $q$ n'est pas unique, il faut comparer plusieurs modèles
- Le premier critère pour juger de la qualité d'un modèle est la blancheur du résidu obtenu (voir la section définitons)


## Modèle ARMA - Série des taux de chômage aux USA

- Nous avons vu que la série des taux de chômage aux USA peut-être considérée comme stationnaire
- On commence par examiner sa fonction d'autocorrélation
```{r, echo=TRUE, out.width="70%", fig.align="center"}
nelplo1909p %>% filter(key=="unemp") %>% ACF(value) %>% autoplot()
```

## Fonction d'autocorrélation partielle

- Le coefficient d'autocorrélation partielle $\phi k,k$ représente l’apport d’explication de $y_{t-k}$
à $y_t$, **toutes choses égales par ailleurs** ()
- La fonction d'autocorrélation partielle permet d'identifier l'ordre $p$ d'un processus AR(p)

## Fonction d'autocorrélation partielle avec `PACF()`

```{r, out.width="70%", fig.align="center"}
nelplo1909p %>% filter(key=="unemp") %>%
  PACF(value) %>% autoplot()
```

## Estimation du modèle (1)

- On commence par un modèle ARMA(2,2)
- L'erreur standard est élevée par rapport à la valeur du coefficient
```{r}
regdata <- nelplo1909p %>% filter(key=="unemp")
arma.mod1 <- arima(regdata[,"value"], c(2,0,2))
summary(arma.mod1)
```

## Estimation du modèle (2)

- On ajuste un modèle ARMA(1,1)
- L'erreur standard des coefficients a diminué fortement
```{r}
regdata <- nelplo1909p %>% filter(key=="unemp")
arma.mod2 <- arima(regdata[,"value"], c(1,0,1)) 
summary(arma.mod2)
```

## Comparaison des modèles

- On retient le second modèle avec moins de paramètres (p=1 et q=1) 
```{r}
AIC(arma.mod1, arma.mod2)
```


## Utilisation de la fonction `auto.arima`

- La fonction `auto.arima()` recherche le meilleur modèle en utilisant un critère d'information
- Le résultat confirme le choix du modèle ARMA(1,1)
```{r}

nelplo.arma <- auto.arima(regdata)
nelplo.arma
```

## Diagnostic du modèle - Préparation

- On prépare un data frame avec les valeurs observées, les valeurs prédites par le modèle, et les résidus
```{r}
nelplo.arma.diag <- data.frame(regdata, prediction=nelplo.arma$fitted, residus=nelplo.arma$residuals)
nelplo.arma.diag
```



## Diagnostic du modèle - Valeurs observées x prédites

```{r, out.width="70%", fig.align='center'}
nelplo.arma.diag %>% 
  pivot_longer(cols=3:5) %>% 
  filter(name %in% c("prediction", "value")) %>%
  ggplot() + geom_line(aes(x=index, y=value, color=name))
```

## Diagnostic du modèle - Résidus

```{r}
nelplo.arma.diag %>% ggplot() + 
  geom_point(aes(x=index, y=residus))
```
## Diagnostic du modèle - Blancheur des résidus

- On teste la blancheur des résidus aux retards 1 à 3
- Toutes les p-valeur sont largement supérieure à 0.05, on peut conclure à la blancheur des résidus
```{r}
lags <- 1:3
pval <- NULL
for (l in lags) {
  pval <-c(pval, box_pierce(nelplo.arma.diag$residus, lag=l)["bp_pvalue"])
}
res <- data.frame(lags, pval)
res
```

## Projection

- On peut utiliser la fonction générique `forecast` pour la projection du taux de chômage  
```{r}
forecast(nelplo.arma, h=10)
```

## Projection - Graphique

```{r, out.width="70%", fig.align="center"}
plot(forecast(nelplo.arma, h=10))
```


# Modèlisation de séries non-stationnaires

## Modèle ARIMA

- Un modèle ARIMA(p,d,q) est un modèle ARMA sur la série différenciée
- $d$ est l'ordre de différentiation
- L'estimation d'un modèle ARIMA revient à un modèle ARMA sur la série différenciée


## Identification d'un modèle ARIMA

- Soit la trajectoire observée $y_1, \ldots , y_t$ d’une série $y_t$ , éventuellement obtenue
après transformation d’une série initiale par passage en log
- La série n’est pas stationnaire et on veut lui ajuster un modèle ARIMA(p, d, q) (on ne traite pas ici des séries présentant une saisonnalité)
- Une fois $d$ choisis, on est ramené à l’identiﬁcation d’un ARMA sur la série diﬀérenciée 
- Pour choisir $d$, on peut tester l’hypothèse d = 1 contre d = 0 avec un test ADF
- On peut également comparer les modèles avec et sans diﬀérenciation à l'aide d'un critère d’information ou de la valeur prédictive

## Modèle ARIMA - Série des dépenses aggégées de consommation aux USA 

- Nous avons vu que cette série (après transformation logarithmique) est non stationnaire et qu'elle possède un trand stochastique
- On peut donc la différencier puis ajuster un modèle ARIMA
- La série différenciée a été calculée précédemment
```{r}
USExp_diff
```


## Examen de la fonction d'autocorrélation

```{r, out.width="70%", fig.align="center"}
USExp_diff %>% as_tsibble(index=time) %>% ACF(diff) %>% autoplot()
```

## Examen de la fonction d'autocorrélation partielle

```{r, out.width="70%", fig.align="center"}
USExp_diff %>% as_tsibble(index=time) %>% PACF(diff) %>% autoplot()
```

## Utilisation de la fonction `auto.arima`

```{r}
regdata <- USExp[,"logExp"]
arimod <- auto.arima(regdata)
arimod
```

## Modèle ARIMA - Prédiction

- La fonction `forecast()` produit les prédiction à partir du modèle sélectionné, avec un intervalle de confiance
```{r}
forecast(arimod, h=24)
```


## Modèle ARIMA - Prédiction - Graphique

```{r, out.width="70%", fig.align="center"}
plot(forecast(arimod, h=24))
```


## Exercice

- Ajustez un modèle ARMA ou ARIMA sur la série du PIB français
- Prédisez le PIB pour les trimestres à venir


# Modèles multivariés

## Série multivariée et processus VAR

- Les modèles VAR (Vector Autoregression) sont l'extension des modèles AR à des séries multivariées
- L'évolution d'une série est modélisée par ses valeurs passées et celles des autres séries
- La série multivariée $y_t=(y_{1t}, \ldots, y_{kt}, \ldots, y_{Kt}), k=1,\ldots, K$ contient $K$ variables
- Un processus VAR(p) est défini par: $$y_t = A_1 y_{t-1} + \ldots + A_p y_{t-p} + \mu_t$$
avec $E(\mu_t)=0$
- $A_i$ est une matrice de $(K \times K)$ coefficients, $i=1, \ldots, p$

## Série multivariée - Données `Canada`

- On utilise la librairie `vars` (voir l'article [ici](https://www.jstatsoft.org/article/view/v027i04))
- Les séries utilisées représentent des indicateurs macro-économiques du marché du travail au Canada (données OCDE):
  - `prod`: productivité du travail (log différence entre le GDP et nombre d'actifs)
  - `e`: nombre d'actifs (employment)
  - `U`: taux de chômage (unemployment rate)
  - `rw`: log de l'index des salaires réels (real wage index)
- Les séries s'étendent du premier trimestre 1980 aux quatrième trimestre 2004  

## Série multivariée - Données `Canada`

- On charge la librairie et les données (objet `mts`)
```{r, message=FALSE}
library("vars")
data("Canada")
window(Canada, start=c(1980,1), end=c(1981,4))
```


## Série multivariée - Graphique 

```{r, out.witdh="70%", fig.align='center'}
plot(Canada, nc = 2, xlab = "")
```
## Série multivariée - Analyse préliminaire

- Le modèle VAR ne permet de modéliser que des séries stationnaires
- Un test de stationarité ADF (Augmented Dickey-Fuller) est réalisé sur les 4 séries
- Toutes les séries sont rendues stationnaires avec une intégration d'ordre 1

## Test de stationarité sur la série `prod`

- La série `prod` présente une tendance, on utilise `type="trend"`
- La valeur `tau3` est supérieure à la valeur critique à 10%, on rejette H0, la série n'est pas stationnaire
```{r}
summary(ur.df(Canada[, "prod"], type = "trend", lags = 2))
```
## Série multivariée - Test de stationarité sur la série `prod` intégrée

- On vérifie que la série est stationnaire après différentiation

```{r}
summary(ur.df(diff(Canada[, "prod"]), type = "drift", lags = 1))
```
## Sélection du lag

- La longueur optimale du lag est déterminée avec la fonction `VARselect`
- La valeur optimale selon le critère AIC est de 3

```{r}
VARselect(Canada, lag.max = 8, type = "both")
```

## Estimation du modèle

- On commence par un modèle d'ordre $p=1$
- `Type = "both"` indique qu'on inclut la constante et la tendance dans le modèle
```{r}
p1ct <- VAR(Canada, p = 1, type = "both")
```
## Equation pour la série `e`

```{r}
summary(p1ct, equation = "e")
```

## Graphique valeurs prédites et résidus

```{r, out.width="70%", fig.align="center"}
plot(p1ct, names = "e")
```
